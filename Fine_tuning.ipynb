{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Niv4cYXgrNvD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import random\n",
        "import os\n",
        "from scipy.io import loadmat\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.cm as cm\n",
        "import time\n",
        "import scipy.io as io\n",
        "\n",
        "import skimage.measure\n",
        "from keras.initializers import RandomNormal\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from keras.optimizers import SGD\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import model_from_json\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.applications.mobilenetv2 import preprocess_input\n",
        "\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(16)\n",
        "tf.random.set_seed(16)\n",
        "random.seed(16)"
      ],
      "metadata": {
        "id": "Z4aezQN8rUAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Images from Mall dataset\n",
        "\n",
        "mall_gt = io.loadmat(\".../mall_dataset/mall_gt.mat\")\n",
        "mall_head_pos = mall_gt['frame'][0]\n",
        "\n",
        "path_frames_mall = '.../frames224/'\n",
        "mall_names = sorted([file for file in os.listdir(path=path_frames_mall) if file.endswith('.jpg')])\n",
        "\n",
        "Mall_imgs = []\n",
        "Mall_labels = []\n",
        "\n",
        "i=0\n",
        "\n",
        "for img_name in sorted(os.listdir(path = path_frames_mall)):\n",
        "    i=i+1\n",
        "    if img_name in mall_names:\n",
        "        img = preprocess_input(cv2.cvtColor(cv2.imread(path_frames_mall + img_name), cv2.COLOR_BGR2RGB))\n",
        "        Mall_imgs.append(img)\n",
        "        Mall_labels.append(len(mall_head_pos[i-1][0][0][0]))\n",
        "\n",
        "Mall_imgs = np.asarray(Mall_imgs)\n",
        "Mall_labels = np.asarray(Mall_labels)"
      ],
      "metadata": {
        "id": "5amuZ33PrXUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Images from Shanghai dataset\n",
        "\n",
        "Train_imgs_B = []\n",
        "Train_labels_B = []\n",
        "\n",
        "path_images = \".../shanghai224/images224/\"\n",
        "path_heads = \".../shanghai224/ground_truth/\"\n",
        "\n",
        "\n",
        "for img_name, heads_name in zip(sorted(os.listdir(path = path_images)), sorted(os.listdir(path = path_heads))):\n",
        "\n",
        "    img = preprocess_input(cv2.cvtColor(cv2.imread(path_images + img_name), cv2.COLOR_BGR2RGB))\n",
        "    Train_imgs_B.append(img)\n",
        "\n",
        "    Train_labels_B.append(len(loadmat(path_heads + heads_name)['image_info'][0, 0][0, 0][0]))\n",
        "\n",
        "\n",
        "Train_imgs_B = np.asarray(Train_imgs_B)\n",
        "Train_labels_B = np.asarray(Train_labels_B)"
      ],
      "metadata": {
        "id": "qx1wTs99rjC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights_dir = 'kaggle/working/weights'\n",
        "os.makedirs(weights_dir, exist_ok=True)\n",
        "\n",
        "# Model\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "predictions = Dense(1, activation='linear')(x)\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4, decay=5e-4), loss=\"mae\", metrics=[\"mae\"])\n",
        "\n",
        "# Data Augmentation\n",
        "#datagen = ImageDataGenerator(\n",
        "#    rotation_range=20,\n",
        "#    width_shift_range=0.2,\n",
        "#    height_shift_range=0.2,\n",
        "#    shear_range=0.2,\n",
        "#    zoom_range=0.2,\n",
        "#    horizontal_flip=True,\n",
        "#    fill_mode='nearest',\n",
        "#    validation_split=0.2)  # 20% validation split\n",
        "\n",
        "checkpoint_filepath = os.path.join(weights_dir, 'mobilenet_crowd_counting_best.weights.h5')\n",
        "model_checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_mae',\n",
        "    mode='min',\n",
        "    save_best_only=True,\n",
        "    verbose=1)\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_mae', patience=10, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_mae', factor=0.2, patience=5, min_lr=1e-6)\n",
        "\n",
        "# Training on Shanghai dataset\n",
        "history = model.fit(Train_imgs_B, Train_labels_B,\n",
        "                    batch_size=16, epochs=200,\n",
        "                    validation_split=0.1,\n",
        "                    callbacks=[model_checkpoint_callback, early_stopping, reduce_lr])\n",
        "\n",
        "final_weights_path = os.path.join(weights_dir, 'mobilenet_crowd_counting_final.weights.h5')\n",
        "model.save('mobilenet_crowd_counting_final.weights.h5')"
      ],
      "metadata": {
        "id": "Lna6S4TTrt8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Mall_imgs_train, Mall_imgs_test, Mall_labels_train, Mall_labels_test = train_test_split(\n",
        "    Mall_imgs, Mall_labels, test_size=0.2, random_state=16\n",
        ")\n",
        "\n",
        "# Loading weights obtained by training the model on Shanghai dataset\n",
        "model.load_weights(checkpoint_filepath)\n",
        "\n",
        "# Training on Mall dataset\n",
        "history_mall = model.fit(Mall_imgs_train, Mall_labels_train,\n",
        "                         batch_size=16, epochs=100,\n",
        "                         validation_split=0.1,\n",
        "                         callbacks=[model_checkpoint_callback, early_stopping, reduce_lr])\n",
        "\n",
        "final_weights_path_mall = os.path.join(weights_dir, 'mobilenet_crowd_counting_final_mall.weights.h5')\n",
        "model.save_weights(final_weights_path_mall)"
      ],
      "metadata": {
        "id": "J6MWOd8Sr-0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, Test_imgs, Test_labels, file_parameters_path):\n",
        "    model.load_weights(file_parameters_path)\n",
        "\n",
        "    mae_error = 0\n",
        "    mse_error = 0\n",
        "    mape_error = 0\n",
        "\n",
        "\n",
        "    N = len(Test_labels)\n",
        "    for i, (x, y) in enumerate(zip(Test_imgs, Test_labels)):\n",
        "\n",
        "        print(i)\n",
        "\n",
        "        mae_error += np.abs(np.sum(model.predict(np.reshape(np.asarray(x),(1, x.shape[0], x.shape[1], x.shape[2])))) - y)\n",
        "        mse_error += np.square(np.sum(model.predict(np.reshape(np.asarray(x),(1, x.shape[0], x.shape[1], x.shape[2])))) - y)\n",
        "        mape_error += np.abs((np.sum(model.predict(np.reshape(np.asarray(x),(1, x.shape[0], x.shape[1], x.shape[2])))) - y) / y)* 100\n",
        "\n",
        "    return mae_error/N, np.sqrt(mse_error/N), mape_error/N\n",
        "\n",
        "# Test on Mall dataset\n",
        "mae, mse, mape  = test_model(model, Mall_imgs[:1200], Mall_labels[:1200], checkpoint_filepath)\n",
        "print(f\"Mean Absolute Error: {mae}, Mean Square Error: {mse}, Mean Absolute Percentage Error:{mape}\")"
      ],
      "metadata": {
        "id": "o6xU5ATFsX1y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}